id,chapter,title,content
13,5-1,웹스크랩퍼-Request사용법,웹 스크랩퍼는 웹사이트상 원하는 위치에서 데이터를 추출하여 수집하는 프로그램을 의미한다.\n이때 주로 두가지 라이브러리를 활용\n첫번째는 requests 라이브러리로\nurl을 이용하여 사이트에 접근하여 결과값을 리턴받는 역할을 합니다.\n기본 라이브러리가 아니기 때문에 따로 설치가 필요하며\npip install requests 명령어를 통해서 패키지를 설치할 수 있다.\n\nimport requests\nURL = "https://~~~~~" \nresult= requests.get(URL) \nprint(result)\n\nimport – 파이썬에 내장되어있는 라이브러리를 불러오는 함수\nrequests.get() – request 라이브러리에서 get이라는 함수를 사용하느것으로 HTML코드를 가져오는 것\nrequest의 대표적인 메소드로는 get, post, put, head, patch, delete, option이 있다.\nGet – get 메소드는 요청 시 GET 방식으로 요청되며 params 매개변수를 지원한다.\nPost – post 메소드는 요청 시 POST 방식으로 요청되며 data, json 매개변수가 존재한다. data, json 두 개의 매개변수는 비슷해 보이지만 요청할 때 헤더의 Content-Type이 달라진다.\nRequest의 응답 객체로는 status_code, raise_for_status, json, text, content, encoding이 있다.\nstatus_code	응답 상태를 확인할 수 있습니다. 응답의 성공한경우 200을 반환한다.\nRaise_for_status \n200 (OK 코드)이 아닌 경우 에러 raie한다\nheaders		headers정보를 확인한다.\ncookies		cookies정보를 확인한다.\nencoding	데이터 인코딩 정보를 확인할 수 있다.\ntext		text 속성을 통해 UTF-8로 인코딩된 문자열을 받을 수 있다.\ncontent		content 속성을 통해 바이너리 타입으로 데이터를 받을 수 있다.\njson()		json response일 경우 딕셔너리 타입으로 바로 변환\n\nimport requests \nurl = "https://blog.naver.com/jinis_stat"\nresponse = requests.get(url)\nprint(response.content) \nprint(response.encoding).\nhtml = response.content.decode('utf-8').strip()\ndecode( ) :  바이트 열을 유니코드로 변환할 때는 디코드 decode 메서드를 사용.\nencode( ) :  유니코드를 바이트 열로 변환할 때는 인코드 encode 메서드를 사용.\nstrip( ) :  문자열에서 양쪽에 있는 연속된 모든 공백을 삭제함
14,5-2,웹스크랩퍼-BeautifulSoup사용법,두번째는 BeautifulSoup4 입니다. \nBeautifulSoup4는 입력된 텍스트를 json 혹은 xml로 구조화해주는 역할을 합니다. 원하는 정보만 추출하기 위해서 페이지에서 리턴되는 자료를 구조화할 필요가 있는데 그 역할을 담당해주는 것입니다.\n이도 request와 마찬가지로 기본 라이브러리가 아니기 때문에 설치가 필요하며\npip install beautifulsoup4 명령어를 통해서 패키지 설치가 가능하다.\nBeautifulSoup가 정보를 분류하는 방식\n분류방식(parser)		특징			설치 여부		속도	\nhtml.parser	 				기본 설치		중간	\nlxml			xml 파일 지원		lxml 설치 필요		매우빠름\nxml			xml 파일 지원		lxml 설치 필요		매우빠름\nhtml5lib			브라우저와 같은 방식	html5lib 설치필요	매우느림\n\nimport requests\nfrom bs4 import BeautifulSoup\nindeed_result = requests.get("https://naver.com")\nindeed_soup = BeautifulSoup(indeed_result.text,"html.parser")\nprint(indeed_result)\nBeautifulSoup()의 경우 rq,get을 통해서 가져온 html을 beatifulsoup라이브러리를 통해서 보기 좋게 정리해주는 함수이다\n\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, "html.parser")\ntext = soup.find('tag', id='id_name').get_text()\nfind( ) :  HTML 페이지에서 원하는 태그(ex] div, span ) 를 다양한 속성(ex] id, class )에 따라 쉽게 필터링함.\nget_text( ) : 현재 문서에서 모든 태그를 제거하고 유니코드 텍스트만 들어 있는 문자열을 반환 함. 쉽게 말해, 원하는 텍스트 블록의 텍스트만 가져오게 하는 옵션임.
15,5-3,코드를 통한 이해,import requests\nfrom bs4 import BeautifulSoup as bs\npage = requests.get("https://library.gabia.com/")\nsoup = bs(page.text, "html.parser")\nelements = soup.select('div.esg-entry-content a > span')\nfor index, element in enumerate(elements, 1):\nprint("{} 번째 게시글의 제목: {}".format(index, element.text))\n\nline 1 ~ 2: 필요한 라이브러리(requests, beautifulsoup)를 import 합니다.\nline 4: requests 를 이용하여 ‘https://library.gabia.com’ 주소로 get 요청을 보내고 응답을 받습니다. 상태 코드와 HTML 내용을 응답받을 수 있습니다.\nline 5: 응답받은 HTML 내용을 BeautifulSoup 클래스의 객체 형태로 생성/반환합니다. BeautifulSoup 객체를 통해 HTML 코드를 파싱하기 위한 여러 가지 기능을 사용할 수 있습니다. (response.text는 응답 받은 내용(HTML)을 Unicode 형태로 반환합니다.)\nline 7: BeautifulSoup 가 제공하는 기능 중 CSS 셀렉터를 이용하여 원하는 정보를 찾을 수 있는 기능입니다. (div.esg-entry-content a > span 은 esg-entry-content 클래스로 설정된 div 태그들의 하위에 존재하는 a 태그, 그 하위에 존재하는 span 태그를 의미합니다.) 이 셀렉터를 이용하면 가비아 라이브러리 홈페이지에 존재하는 포스터들의 제목을 추출할 수 있습니다.\n\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nhtml = urlopen("http://www.naver.com")\nbsObject = BeautifulSoup(html, "html.parser")\nprint(bsObject.head.title)\n\nhead.title – 태그로 구성된 트리에서 title태그만 출력하도록 하는 명령어\n\n\nimport requests\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\npage =\nrequests.get("https://library.gabia.com/")\nsoup = bs(page.text, "html.parser")\nelements = soup.select('div.esg-entry-content a.eg-grant-element-0')\ntitles = []\nlinks = []\nfor index, element in enumerate(elements, 1):\n        titles.append(element.text)\n       \nlinks.append(element.attrs['href'])\ndf = pd.DataFrame()\ndf['titles'] = titles\ndf['links'] = links\ndf.to_excel('./library_gabia.xlsx', sheet_name='Sheet1')\n\nline 3: pandas 라이브러리를 import합니다.\nline 1 ~ 14: 이전에 작성한 코드와 동일합니다. 다만, 이번에는 게시글의 제목, 링크를 출력하지 않고 각각의 배열에 값을 추가합니다.\nline 17 ~ 19: titles 배열과 links 배열의 값으로 Pandas의 DataFrame 을 생성합니다.\nline 21: DataFrame 의 to_excel() 함수를 이용하여 엑셀파일을 작성합니다.\n\n\ndef get_post_list(last_page):\n    posts = []\n    for page in range(last_page):\n        print(f"Getting page {page+1} posts ...")\n        result = rq.get(f"{TISTORY_URL}/?page={page+1}")\n        soup = BeautifulSoup(result.text, "html.parser")\n        post_items = soup.find_all("div", {"class" : "post-item"})\n        for post_item in post_items:\n            post = get_post_info(post_item)\n            posts.append(post)\n    return posts\ndef get_post_info(post_item):\n    title = post_item.find("span", {"class" : "title"}).string.strip()\n    date = post_item.find("span", {"class" : "date"}).string.strip()\n    link = post_item.find("a")["href"]\n    return {"title" : title,\n                "date" : date,\n                "link" : f"{TISTORY_URL}{link}"}\n\nget_post_list 함수에서는, last_page의 수만큼 URL을 수정하며 rq.get을 호출합니다.\n이를 통해 class명이 post-item이라는 div들을 list형태로 post_items라는 변수로 저장합니다.\n*find와 find_all의 차이점 : find의 경우는 제일 처음 발견되는 한 개를 반환하는 반면에, find_all의 경우 해당하는 모든 요소를 리스트 형태로 출력하게 됩니다.\n이 개수만큼 for문을 사용하여 새로 정의한 get_post_info라는 함수를 call 하여, 각각의 post에 대한 정보를 리턴 받고, 이를 posts라고 정의한 리스트형 변수에 append(추가) 해 줍니다.\nget_post_info 함수에서는, post_item이라는 div를 입력받아서, 이 포스팅의 제목, 게시날짜, 링크를 묶어서 dictionary형태로 반환해주는 함수입니다.\n\n\ndef save_post_to_csv(posts):\n    file = open("posts.csv", mode = "w")\n    writer = csv.writer(file)\n    writer.writerow(["제목", "게시 날짜", "링크"])\n    for post in posts:\n        writer.writerow(list(post.values()))\n    print("Save Finished!")\nsave_post_to_csv라는 함수는, posts라는 리스트를 입력받아서, 이 입력을 '제목', '게시 날짜', '링크' 순으로. csv 파일에 넣어주는 역할을 합니다.
